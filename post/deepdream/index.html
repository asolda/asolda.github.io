<html lang="en">
<head>

  
  <meta charset="utf-8">
  <title>Bake your own Deep Dream</title>
  <meta name="description" content="Bake your own Deep Dream">
  <meta name="author" content="">

  
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://asolda.github.io/css/fonts.css">
  
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">
  

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">

  <link rel="stylesheet" href="https://asolda.github.io/css/custom.css">

  
  
  <link rel="stylesheet" href="https://asolda.github.io/highlight/styles/default.css">
  
  <script src="https://asolda.github.io/highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

</head>
<body>


<div class="header pure-g">
    <div class="pure-u-1-24 pure-u-md-5-24"></div>
    <div class="pure-u-11-12 pure-u-md-5-8">
        <div class="desktop pure-menu pure-menu-horizontal nav-menu">
            
            <a href="https://asolda.github.io/" class="site-title pure-menu-heading">Yet another computer science blog</a>
            <ul class="pure-menu-list">
                
                <li class="pure-menu-item">
                    <a href="https://asolda.github.io/about/" class="pure-menu-link">About</a>
                </li>
            </ul>
        </div>
        <div class="mobile pure-menu nav-menu">
            <a href="/" class="pure-menu-heading" id="toggle-home">Yet another computer science blog</a>
            <a href="#" id="toggle-btn">&#9776;</a>
            <ul class="pure-menu-list" id="toggle-content" style="display:none;">
                
                
                <li class="pure-menu-item">
                    <a href="https://asolda.github.io/about" class="pure-menu-link">About</a>
                </li>
            </ul>
        </div>
    </div>
    <div class="pure-u-1-24 pure-u-md-1-6"></div>
</div>


<div class="pure-g">
    <div class="pure-u-1-24 pure-u-md-5-24"></div>
	<div class="pure-u-11-12 pure-u-md-5-8">
        <div class="post">

            <div class="post-title">
                <p class="footnote">
                    <time class="">2016-12-06</time>
		            
                    
                    

                    

                    
                </p>
                <h1>Bake your own Deep Dream</h1>
            </div>

            <div class="post-content">
                

<p>This article is a follow up to <a href="https://asolda.github.io/post/deepdream/">this article</a>; I suggest you check it out before moving on.</p>

<h1 id="an-article-with-a-purpose">An article with a purpose</h1>

<p><strong>Deep Learning</strong> is, without a doubt, an interesting and fascinating topic.
It can be applied in many different contexts, from image recognition to text processing;
companies like Google and Facebook heavily rely on Deep Learning to offer many of their services.</p>

<p>However, we are not going to understand how DL can integrate inside business logics; that would be constructive and useful, <em>ergo</em> not suited for this blog.</p>

<p>Instead, we are going to take a deeper look at Google <a href="https://en.wikipedia.org/wiki/DeepDream">Deep Dream</a>,
aiming to create this hypnotic images by ourself.
Please note that, jokes aside, this <strong>is</strong> indeed useful, allowing us to better undestand the processes that take place during the classification procedure.</p>

<h2 id="tools">Tools</h2>

<p>Once again, we are going to use <a href="https://keras.io/">Keras</a> on top of <a href="https://www.tensorflow.org/">TensorFlow</a>,
to mantain the code readable and to avoid complications.</p>

<p>We are going to implement our own Deep Dream convnet using the <a href="https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3">pre-trained weights</a> we have already used last time.</p>

<p>The code, however, will be slightly different and we are not reusing the one we wrote last time.</p>

<h1 id="the-coding-bit">The coding bit</h1>

<p>First of, we start with a few utilities:</p>

<pre><code>from __future__ import print_function
from keras.preprocessing.image import load_img, img_to_array
import numpy as np
from scipy.misc import imsave
from scipy.optimize import fmin_l_bfgs_b
import time
import argparse

from keras.applications import vgg16
from keras import backend as K
from keras.layers import Input

parser = argparse.ArgumentParser(description='Deep dream implelentation with TensorFlow')
parser.add_argument('base_image_path', metavar='base', type=str,
                    help='Path to the image to transform.')
parser.add_argument('result_prefix', metavar='res_prefix', type=str,
                    help='Prefix for the saved results.')

args = parser.parse_args()
base_image_path = args.base_image_path
result_prefix = args.result_prefix

N_ITER = 100

# dimensions of the generated picture.
img_width = 450
img_height = 900

# path to the model weights file.
weights_path = 'vgg16_weights.h5'
</code></pre>

<p>These utilities include:</p>

<ul>
<li><p><code>import</code> statements</p></li>

<li><p><strong>Argument</strong> handling: we expect two parameters, the starting image and a prefix to save the output images</p></li>

<li><p>The maximum number of iterations to perform (<code>N_ITER</code>)</p></li>

<li><p><strong>Dimensions</strong> for the generated images</p></li>

<li><p>The <strong>weights</strong> to use for our network (VGG16)</p></li>
</ul>

<p>Loading the model is, like last time, pretty straightforward:</p>

<pre><code># this will contain our generated image
dream = Input(batch_shape=(1,) + img_size)

# build the VGG16 network with our placeholder
# the model will be loaded with pre-trained ImageNet weights
model = vgg16.VGG16(input_tensor=dream,
                    weights='imagenet', include_top=False)
print('Model loaded.')

layer_dict = dict([(layer.name, layer) for layer in model.layers])
</code></pre>

<p>We create a <code>placeholder</code> and we use it as the <code>input_tensor</code> for our network.</p>

<p>The <code>img_size</code> variable is computed this way:</p>

<pre><code>if K.image_dim_ordering() == 'th':
    img_size = (3, img_width, img_height)
else:
    img_size = (img_width, img_height, 3)
</code></pre>

<p>The <code>dim_ordering</code> can either be &ldquo;tf&rdquo; or &ldquo;th&rdquo;.
It tells Keras whether to use Theano or TensorFlow dimension ordering for inputs/kernels/ouputs.</p>

<p>We can now write our loss function:</p>

<pre><code># define the loss
loss = K.variable(0.)
for layer_name in settings['features']:
    # add the L2 norm of the features of a layer to the loss
    assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.'
    coeff = settings['features'][layer_name]
    x = layer_dict[layer_name].output
    shape = layer_dict[layer_name].output_shape
    # we avoid border artifacts by only involving non-border pixels in the loss
    if K.image_dim_ordering() == 'th':
        loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2] - 2, 2: shape[3] - 2])) / np.prod(shape[1:])
    else:
        loss -= coeff * K.sum(K.square(x[:, 2: shape[1] - 2, 2: shape[2] - 2, :])) / np.prod(shape[1:])
</code></pre>

<p>Next step is applying a couple of tweaks to achieve better results:</p>

<ul>
<li><p>A <strong>continuity loss</strong>, to give the image local coherence and avoid messy blurs</p></li>

<li><p>The <strong>L2 norm loss</strong> in order to prevent pixels from taking very high values</p></li>
</ul>

<p>Two lines of code will suffice:</p>

<pre><code># add continuity loss
loss += settings['continuity'] * continuity_loss(dream) / np.prod(img_size)
# add image L2 norm to loss
loss += settings['dream_l2'] * K.sum(K.square(dream)) / np.prod(img_size)
</code></pre>

<p>where <code>continuity_loss</code> is a utility function defined by us:</p>

<pre><code># continuity loss util function
def continuity_loss(x):
    assert K.ndim(x) == 4
    if K.image_dim_ordering() == 'th':
        a = K.square(x[:, :, :img_width - 1, :img_height - 1] -
                     x[:, :, 1:, :img_height - 1])
        b = K.square(x[:, :, :img_width - 1, :img_height - 1] -
                     x[:, :, :img_width - 1, 1:])
    else:
        a = K.square(x[:, :img_width - 1, :img_height-1, :] -
                     x[:, 1:, :img_height - 1, :])
        b = K.square(x[:, :img_width - 1, :img_height-1, :] -
                     x[:, :img_width - 1, 1:, :])
    return K.sum(K.pow(a + b, 1.25))
</code></pre>

<p>Our model is now complete.
You can feel free to further modify the loss as you see fit, to achieve new effects.</p>

<p>Now things can get a little bit trickier:
we need to evaluate our loss and our gradients in one pass, but <code>scipy.optimize</code> requires separate functions for loss and gradients,
and computing them separately would be inefficient.
To solve this we create our own <code>Evaluator</code>:</p>

<pre><code>class Evaluator(object):
    def __init__(self):
        self.loss_value = None
        self.grad_values = None

    def loss(self, x):
        assert self.loss_value is None
        loss_value, grad_values = eval_loss_and_grads(x)
        self.loss_value = loss_value
        self.grad_values = grad_values
        return self.loss_value

    def grads(self, x):
        assert self.loss_value is not None
        grad_values = np.copy(self.grad_values)
        self.loss_value = None
        self.grad_values = None
        return grad_values

evaluator = Evaluator()
</code></pre>

<p>The <code>eval_loss_and_grads</code> functions will then read as follow:</p>

<pre><code># compute the gradients of the dream wrt the loss
grads = K.gradients(loss, dream)

outputs = [loss]
if type(grads) in {list, tuple}:
    outputs += grads
else:
    outputs.append(grads)

f_outputs = K.function([dream], outputs)
def eval_loss_and_grads(x):
    x = x.reshape((1,) + img_size)
    outs = f_outputs([x])
    loss_value = outs[0]
    if len(outs[1:]) == 1:
        grad_values = outs[1].flatten().astype('float64')
    else:
        grad_values = np.array(outs[1:]).flatten().astype('float64')
    return loss_value, grad_values
</code></pre>

<p>All that&rsquo;s left now is to run <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> optimizer over the pixels of the generated image,
in order to minimize the loss:</p>

<pre><code>x = preprocess_image(base_image_path)
for i in range(N_ITER):
    print('Start of iteration', i)
    start_time = time.time()

    # add a random jitter to the initial image. This will be reverted at decoding time
    random_jitter = (settings['jitter'] * 2) * (np.random.random(img_size) - 0.5)
    x += random_jitter

    # run L-BFGS for 7 steps
    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),
                                     fprime=evaluator.grads, maxfun=7)
    print('Current loss value:', min_val)
    # decode the dream and save it
    x = x.reshape(img_size)
    x -= random_jitter
    img = deprocess_image(np.copy(x))
    fname = result_prefix + '_at_iteration_%d.png' % i
    imsave(fname, img)
    end_time = time.time()
    print('Image saved as', fname)
    print('Iteration %d completed in %ds' % (i, end_time - start_time))
</code></pre>

<p>What are we missing? Just a couple of utility functions and a custom configuration (the <code>settings</code> variable) for our network:</p>

<ul>
<li>Here are the functions <code>preprocess_image</code> and <code>deprocess_image</code>:</li>
</ul>

<pre><code># util function to open, resize and format pictures into appropriate tensors
def preprocess_image(image_path):
    img = load_img(image_path, target_size=(img_width, img_height))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg16.preprocess_input(img)
    return img

# util function to convert a tensor into a valid image
def deprocess_image(x):
    if K.image_dim_ordering() == 'th':
        x = x.reshape((3, img_width, img_height))
        x = x.transpose((1, 2, 0))
    else:
        x = x.reshape((img_width, img_height, 3))
    # Remove zero-center by mean pixel
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68
    # 'BGR'-&gt;'RGB'
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype('uint8')
    return x
</code></pre>

<ul>
<li>Next up, a couple of example settings that I like:</li>
</ul>

<pre><code>saved_settings = {
    'acid': {'features': {'block4_conv1': 0.05,
                              'block4_conv2': 0.01,
                              'block4_conv3': 0.01},
                 'continuity': 0.1,
                 'dream_l2': 0.8,
                 'jitter': 5},
    'doggos': {'features': {'block5_conv1': 0.05,
                            'block5_conv2': 0.02},
               'continuity': 0.1,
               'dream_l2': 0.02,
               'jitter': 0},
}

# the settings we will use in this experiment
settings = saved_settings['doggos']
</code></pre>

<p>The results that I&rsquo;ll show below are obtained with the <code>doggos</code> setting.</p>

<h1 id="the-fun-part">The fun part</h1>

<p>To test our model I&rsquo;ve chosen famous paintings and I&rsquo;ve run the code above for an &ldquo;appropriate&rdquo; number of iterations.
I usually prefer images where you can recognize both the original painting and the network&rsquo;s work.</p>

<h2 id="input-1-nascita-di-venere">Input 1: Nascita di Venere</h2>

<p>The <em><a href="https://en.wikipedia.org/wiki/The_Birth_of_Venus">Birth if Venus</a></em> is a painting by Sandro Botticelli generally thought to have been painted in the mid 1480s.
This is an iconic and easily recognizable painting:</p>

<p><a href="/images/venus.jpg"><img src="/images/venus.jpg" style="margin: 0 auto; display: block; width:75%"></a></p>

<p>Here are the results after various number of iterations:</p>

<ul>
<li>After <strong>1</strong> iteration</li>
</ul>

<p><a href="/images/venus-1.png"><img src="/images/venus-1.png" style="margin: 0 auto; display: block; width:60%"></a></p>

<p>The painting is clearly there, almost untouched, but you can already see that weird images are forming.</p>

<ul>
<li>After <strong>5</strong> iterations</li>
</ul>

<p><a href="/images/venus-5.png"><img src="/images/venus-5.png" style="margin: 0 auto; display: block; width:60%"></a></p>

<ul>
<li>After <strong>10</strong> iterations</li>
</ul>

<p><a href="/images/venus-10.png"><img src="/images/venus-10.png" style="margin: 0 auto; display: block; width:60%"></a></p>

<p>Here the painting has been heavily modified, and you can recognize animals that the network was trained to recognize.</p>

<ul>
<li>After <strong>20</strong> iterations</li>
</ul>

<p><a href="/images/venus-20.png"><img src="/images/venus-20.png" style="margin: 0 auto; display: block; width:60%"></a></p>

<ul>
<li>And finally, after <strong>25</strong> iterations</li>
</ul>

<p><a href="/images/venus-25.png"><img src="/images/venus-25.png" style="margin: 0 auto; display: block; width:75%"></a></p>

<p>This is the point where I like it the most, but it&rsquo;s a matter of personal test and you can perform as many iterations as you like.</p>

<h2 id="input-2-creazione-di-adamo">Input 2: Creazione di Adamo</h2>

<p>The <a href="https://en.wikipedia.org/wiki/The_Creation_of_Adam">Creation of Adam</a> is a fresco painting by Michelangelo, which forms part of the Sistine Chapel&rsquo;s ceiling, painted c. 1508–1512.
Yet another very famous panting:</p>

<p><a href="/images/adam.jpg"><img src="/images/adam.jpg" style="margin: 0 auto; display: block; width:75%"></a></p>

<p>Let&rsquo;s see how our network will deface this painting:</p>

<ul>
<li>After <strong>1</strong> iteration:</li>
</ul>

<p><a href="/images/adam-1.png"><img src="/images/adam-1.png" style="margin: 0 auto; display: block; width:60%"></a></p>

<ul>
<li>After <strong>20</strong> iterations:</li>
</ul>

<p><a href="/images/adam-20.png"><img src="/images/adam-20.png" style="margin: 0 auto; display: block; width:60%"></a></p>

<ul>
<li>After <strong>40</strong> iterations:</li>
</ul>

<p><a href="/images/adam-40.png"><img src="/images/adam-40.png" style="margin: 0 auto; display: block; width:60%"></a></p>

<ul>
<li>After <strong>60</strong> iterations:</li>
</ul>

<p><a href="/images/adam-60.png"><img src="/images/adam-60.png" style="margin: 0 auto; display: block; width:75%"></a></p>

<h2 id="other-tests">Other tests</h2>

<p>I&rsquo;ve tested the code on many different paintings and photos, I will just include here the two that I liked the most:</p>

<p><a href="/images/sunflowers.png"><img src="/images/sunflowers.png" style="margin: 0 auto; display: block; width:33%"></a></p>

<p><a href="/images/river.png"><img src="/images/river.png" style="margin: 0 auto; display: block; width:60%"></a></p>

<p>My bet is you will recognize this paintings.</p>

<h1 id="summary">Summary</h1>

<p>Deep Dream help us understand and visualize how neural networks are able to carry out difficult classification tasks, improve network architecture, and check what the network has learned during training.
It also makes us wonder whether neural networks could become a tool for artists, a new way to remix visual concepts, or perhaps even shed a little light on the roots of the creative process in general.</p>

                <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'asolda-github-io';
    var disqus_identifier = 'https:\/\/asolda.github.io\/post\/deepdream\/';
    var disqus_title = 'Bake your own Deep Dream';
    var disqus_url = 'https:\/\/asolda.github.io\/post\/deepdream\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
            </div>
        </div>
	</div>
    <div class="pure-u-1-24 pure-u-md-1-6"></div>
</div>

<div class="footer pure-g">
    <div class="pure-u-1-24 pure-u-md-5-24"></div>
    <div class="pure-u-11-12 pure-u-md-5-8">
        <div class="pure-menu pure-menu-horizontal footer-content">
            <ul>
                <li class="pure-menu-heading" id="foot-name">:</li>

                

                

                

                

                

            </ul>
            <a href="#" class="pure-menu-heading pull-right" id="gototop-btn">↑↑</a>
        </div>
	  </div>
      <div class="pure-u-1-24 pure-u-md-1-6"></div>
</div>


<script src="https://asolda.github.io/js/jquery.min.js" type="text/javascript"></script>
<script src="https://asolda.github.io/js/jquery.timeago.js" type="text/javascript"></script>
<script type="text/javascript">
  $(function(){
    $("time.timeago").timeago();
  })
  $("#toggle-btn").click(function(){
    $("#toggle-content").toggle();
    if($(this).html() === "☰") {
        $(this).html("X")
    } else {
        $(this).html("☰")
    }
  });
  $(window).resize(function(){
    if(window.innerWidth > 768) {
      $(".desktop").removeAttr("style");
    }
  });
</script>

</body>
</html>

