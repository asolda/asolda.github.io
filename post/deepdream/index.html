<html lang="en">
<head>

  
  <meta charset="utf-8">
  <title>The day I found out my computer does drugs</title>
  <meta name="description" content="The day I found out my computer does drugs">
  <meta name="author" content="">

  
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://asolda.github.io/css/fonts.css">
  
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">
  

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">

  <link rel="stylesheet" href="https://asolda.github.io/css/custom.css">

  
  
  <link rel="stylesheet" href="https://asolda.github.io/highlight/styles/default.css">
  
  <script src="https://asolda.github.io/highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

</head>
<body>


<div class="header pure-g">
    <div class="pure-u-1-24 pure-u-md-5-24"></div>
    <div class="pure-u-11-12 pure-u-md-5-8">
        <div class="desktop pure-menu pure-menu-horizontal nav-menu">
            
            <a href="https://asolda.github.io/" class="site-title pure-menu-heading">Yet another computer science blog</a>
            <ul class="pure-menu-list">
                
                <li class="pure-menu-item">
                    <a href="https://asolda.github.io/about/" class="pure-menu-link">About</a>
                </li>
            </ul>
        </div>
        <div class="mobile pure-menu nav-menu">
            <a href="/" class="pure-menu-heading" id="toggle-home">Yet another computer science blog</a>
            <a href="#" id="toggle-btn">&#9776;</a>
            <ul class="pure-menu-list" id="toggle-content" style="display:none;">
                
                
                <li class="pure-menu-item">
                    <a href="https://asolda.github.io/about" class="pure-menu-link">About</a>
                </li>
            </ul>
        </div>
    </div>
    <div class="pure-u-1-24 pure-u-md-1-6"></div>
</div>


<div class="pure-g">
    <div class="pure-u-1-24 pure-u-md-5-24"></div>
	<div class="pure-u-11-12 pure-u-md-5-8">
        <div class="post">

            <div class="post-title">
                <p class="footnote">
                    <time class="">2016-12-05</time>
		            
                    
                    

                    

                    
                </p>
                <h1>The day I found out my computer does drugs</h1>
            </div>

            <div class="post-content">
                

<h1 id="exploring-the-unknown">Exploring the unknown</h1>

<p>In this article we will talk about <strong>deep convolutional neural network</strong>, a.k.a. how computers are going to rule the world.
Someday. Eventually. Definetely not right now, mostly because I&rsquo;ve come to the realization that my computer is constantly high on some weird acid.
More on this later. For now, let&rsquo;s just understand that we are going to talk about complex topics, that this article isn&rsquo;t meant for everyone out there and that I won&rsquo;t have the time to explain everithing I say&hellip;
I just hope you&rsquo;ll follow through.</p>

<h2 id="what-are-we-doing">What are we doing?</h2>

<p>We want to take a look at what deep convolutional neural networks (convnets) really learn, and how they understand the images we feed them.</p>

<p>We will use <a href="https://keras.io/">Keras</a>, a Deep Learning library for <a href="http://deeplearning.net/software/theano/">Theano</a> and <a href="https://www.tensorflow.org/">TensorFlow</a>.</p>

<p><strong>Keras</strong> is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano.
It was developed with a focus on enabling fast experimentation, allowing researchers to &ldquo;<em>go from idea to result with the least possible delay</em>&rdquo;.
It is a really interesting project and can be extremely useful.
In this post I will be using <strong>Keras</strong> on top of <strong>TensorFlow</strong>, but the code can be seamlessy adapted to work on top of Theano if you prefer.</p>

<p>We will use Keras to visualize inputs that maximize the activation of the filters in different layers of the VGG16 architecture, trained on <a href="http://www.image-net.org/">ImageNet</a>.</p>

<h1 id="visualizing-convnet-filters">Visualizing convnet filters</h1>

<p>VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the <a href="http://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group</a> from Oxford, who developed it.
It was used to win the <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">ILSVR (ImageNet) competition in 2014</a>.
To this day is it still considered to be an excellent vision model, although it has been somewhat outperformed by more recent advances (Inception, ResNet).</p>

<p><a href="http://www.lorenzobaraldi.com/">Lorenzo Baraldi</a> ported the pre-trained Caffe version of VGG16 (as well as VGG19) to a Keras weights file, so we will just load that to do our experiments. You can download the weight file <a href="https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3">here</a>.</p>

<p>Loading the model is made easy by Keras:</p>

<pre><code># build the VGG16 network with ImageNet weights
model = vgg16.VGG16(weights='imagenet', include_top=False)
print('Model loaded.')

model.summary()
</code></pre>

<p>Yup, it&rsquo;s <strong>that</strong> easy. Moving along, we prepare a couple of utilities that we will need later:</p>

<ul>
<li>A <strong>placeholder</strong> (<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html">read the doc</a> if you&rsquo;re confused):</li>
</ul>

<pre><code># this is the placeholder for the input images
input_img = model.input
</code></pre>

<ul>
<li>A <strong>dictionary</strong> that we will use to create <em>ad-hoc</em> loss functions:</li>
</ul>

<pre><code># get the symbolic outputs of each &quot;key&quot; layer (we gave them unique names).
layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])
</code></pre>

<ul>
<li>A utility function to <strong>normalize</strong> a Tensor (using its L2 Norm)</li>
</ul>

<pre><code>def normalize(x):
    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)
</code></pre>

<p>Moving to the interesting part of the code, it&rsquo;s now time to scan through the filters, compute the loss functions and visualize the most interesting filters.
The scanning process is pretty straightforward:</p>

<pre><code>kept_filters = []
for filter_index in range(0, 200):
    # we only scan through the first 200 filters,
    # but there are more (512)
    print('Processing filter %d' % filter_index)
    start_time = time.time()

    # we build a loss function that maximizes the activation
    # of the nth filter of the layer considered
    layer_output = layer_dict[layer_name].output
    if K.image_dim_ordering() == 'th':
        loss = K.mean(layer_output[:, filter_index, :, :])
    else:
        loss = K.mean(layer_output[:, :, :, filter_index])

    # we compute the gradient of the input picture wrt this loss
    grads = K.gradients(loss, input_img)[0]

    # normalization trick: we normalize the gradient
    grads = normalize(grads)

    # this function returns the loss and grads given the input picture
    iterate = K.function([input_img], [loss, grads])

    # step size for gradient ascent
    step = 1.

    # we start from a gray image with some random noise
    if K.image_dim_ordering() == 'th':
        input_img_data = np.random.random((1, 3, img_width, img_height))
    else:
        input_img_data = np.random.random((1, img_width, img_height, 3))
    input_img_data = (input_img_data - 0.5) * 20 + 128

    # we run gradient ascent for 20 steps
    for i in range(20):
        loss_value, grads_value = iterate([input_img_data])
        input_img_data += grads_value * step

        print('Current loss value:', loss_value)
        if loss_value &lt;= 0.:
            # some filters get stuck to 0, we can skip them
            break

    # decode the resulting input image
    if loss_value &gt; 0:
        img = deprocess_image(input_img_data[0])
        kept_filters.append((img, loss_value))
    end_time = time.time()
    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))
</code></pre>

<p>The code is well commented and Keras makes everything extremely linear and readable.
Just a couple of notes:</p>

<ul>
<li><p>The <code>loss function</code> is easy, but effective for our purpose.</p></li>

<li><p>The &ldquo;<em>normalization trick</em>&rdquo; is worth pointing out, start your research from <a href="http://stats.stackexchange.com/questions/22568/difference-in-using-normalized-gradient-and-gradient">here</a> if you are interested.</p></li>

<li><p>The <code>deprocess_image</code> function can be a little bit tricky, but here you are:</p></li>
</ul>

<pre><code># util function to convert a tensor into a valid image
def deprocess_image(x):
    # normalize tensor: center on 0., ensure std is 0.1
    x -= x.mean()
    x /= (x.std() + 1e-5)
    x *= 0.1

    # clip to [0, 1]
    x += 0.5
    x = np.clip(x, 0, 1)

    # convert to RGB array
    x *= 255
    if K.image_dim_ordering() == 'th':
        x = x.transpose((1, 2, 0))
    x = np.clip(x, 0, 255).astype('uint8')
    return x
</code></pre>

<ul>
<li>The <code>layer_name</code> variable should look like this:</li>
</ul>

<pre><code># the name of the layer we want to visualize
# (see model definition at keras/applications/vgg16.py)
layer_name = 'block5_conv1'
</code></pre>

<p>We can now move on to select the &ldquo;best&rdquo; layers;
the filters that have the highest loss are supposed to be the best looking:</p>

<pre><code># we will stich the best 64 filters on a 8 x 8 grid.
n = 8

# we will only keep the top 64 filters.
kept_filters.sort(key=lambda x: x[1], reverse=True)
kept_filters = kept_filters[:n * n]

# build a black picture with enough space for
# our 8 x 8 filters of size 128 x 128, with a 5px margin in between
margin = 5
width = n * img_width + (n - 1) * margin
height = n * img_height + (n - 1) * margin
stitched_filters = np.zeros((width, height, 3))

# fill the picture with our saved filters
for i in range(n):
    for j in range(n):
        img, loss = kept_filters[i * n + j]
        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,
                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img

# save the result to disk
imsave('stitched_filters_%dx%d.png' % (n, n), stitched_filters)
</code></pre>

<p>With</p>

<pre><code># dimensions of the generated pictures for each filter.
img_width = 128
img_height = 128
</code></pre>

<p>the image we are getting is this:</p>

<p><img src="/images/filters.png" alt="block5_conv1" /></p>

<p>We can obviously use this code to visualize different filters for each layer.
If you do this, you will notice that the first layers basically just encode direction and color.
These direction and color filters then get combined into basic grid and spot textures.
These textures gradually get combined into increasingly complex patterns.</p>

<p>In the highest layers (like the one above) we start to recognize textures similar to that found in the objects that network was trained to classify, such as feathers, eyes, etc.</p>

<p>Another fun thing to do is to apply these filters to photos (rather than to noisy all-gray inputs).
This is the principle of <a href="https://en.wikipedia.org/wiki/DeepDream">Deep Dreams</a>, popularized by Google last year.</p>

<p>We will learn how to implement Deep Dream in the next article, in the meanwhile you can have fun visualizing different layers and using photos as a starting point for filters visualization.</p>

<h1 id="summary">Summary</h1>

<p>The key message here is that <strong>convnets don&rsquo;t understand concepts</strong> just because thay are able to classify objects.
The visual decomposition of visual space learned by a convnet is (apparently) analogous to what the human visual cortex does.
It may or may not be true. So what do they understand? Basically two things:</p>

<ul>
<li><p>They understand a <strong>decomposition of their visual input space</strong> as a hierarchical-modular network of convolution filters.</p></li>

<li><p>They understand a <strong>probabilitistic mapping between certain combinations of these filters</strong> and a set of arbitrary labels.</p></li>
</ul>

<p>Naturally, this does not qualify as &ldquo;seeing&rdquo; in any human sense, and it doesn&rsquo;t mean we have solved computer vision.</p>

<p>That said, visualizing what convnets learn is quite fascinating.
Deep learning may not be intelligence in any real sense, but it&rsquo;s still working considerably better than anybody could have anticipated just a few years ago.
Guess it&rsquo;s about time someone figures out <strong>why</strong>&hellip;</p>

            </div>
            
        </div>
	</div>
    <div class="pure-u-1-24 pure-u-md-1-6"></div>
</div>
<div class="footer pure-g">
    <div class="pure-u-1-24 pure-u-md-5-24"></div>
    <div class="pure-u-11-12 pure-u-md-5-8">
        <div class="pure-menu pure-menu-horizontal footer-content">
            <ul>
                <li class="pure-menu-heading" id="foot-name">:</li>

                

                

                

                

                

            </ul>
            <a href="#" class="pure-menu-heading pull-right" id="gototop-btn">↑↑</a>
        </div>
	  </div>
      <div class="pure-u-1-24 pure-u-md-1-6"></div>
</div>


<script src="https://asolda.github.io/js/jquery.min.js" type="text/javascript"></script>
<script src="https://asolda.github.io/js/jquery.timeago.js" type="text/javascript"></script>
<script type="text/javascript">
  $(function(){
    $("time.timeago").timeago();
  })
  $("#toggle-btn").click(function(){
    $("#toggle-content").toggle();
    if($(this).html() === "☰") {
        $(this).html("X")
    } else {
        $(this).html("☰")
    }
  });
  $(window).resize(function(){
    if(window.innerWidth > 768) {
      $(".desktop").removeAttr("style");
    }
  });
</script>

</body>
</html>

