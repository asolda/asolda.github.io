<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on Yet another computer science blog</title>
    <link>https://asolda.github.io/post/index.xml</link>
    <description>Recent content in Post-rsses on Yet another computer science blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Dec 2016 10:42:23 +0100</lastBuildDate>
    <atom:link href="https://asolda.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>An introduction to Deep Learning: part 2</title>
      <link>https://asolda.github.io/post/deeplearning-2/</link>
      <pubDate>Fri, 09 Dec 2016 10:42:23 +0100</pubDate>
      
      <guid>https://asolda.github.io/post/deeplearning-2/</guid>
      <description>

&lt;h1 id=&#34;where-we-left&#34;&gt;Where we left&lt;/h1&gt;

&lt;p&gt;In the &lt;a href=&#34;https://asolda.github.io/post/deeplearning-1/&#34;&gt;first part&lt;/a&gt; of this article we have built a complete &lt;strong&gt;Linear Classifier&lt;/strong&gt;;
it&amp;rsquo;s now time to move on, and get to the interesting bit: &lt;strong&gt;Deep Learning&lt;/strong&gt; is about to kick in.&lt;/p&gt;

&lt;h2 id=&#34;actually-part-1-5&#34;&gt;Actually, part 1.5&lt;/h2&gt;

&lt;p&gt;Sorry about that, but we are not entirely done with our classifier.&lt;/p&gt;

&lt;p&gt;Sure, training &lt;strong&gt;Logistic Regression&lt;/strong&gt; using &lt;strong&gt;Gradient Descent&lt;/strong&gt; is great: for one thing you are directly optimizing the error measure that you care about; that is always a great idea.
In practice, a lot of &lt;strong&gt;Machine Learning&lt;/strong&gt; &lt;em&gt;research&lt;/em&gt; is about designing the right loss function to optimize.
The main issue with these models is that they are very &lt;strong&gt;difficult&lt;/strong&gt; to &lt;strong&gt;scale&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;stochastic-gradient-descent&#34;&gt;Stochastic Gradient Descent&lt;/h3&gt;

&lt;p&gt;The problem is simple: you need to compute this gradient:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://asolda.github.io/images/formulas/7.png&#34; style=&#34;margin: 0 auto; display: block; width:33%&#34;&gt;&lt;/p&gt;

&lt;p&gt;A &amp;ldquo;rule of thumb&amp;rdquo; is that if computing your &lt;strong&gt;Loss&lt;/strong&gt; takes &lt;em&gt;n&lt;/em&gt; floating point operations, computing its gradient takes about &lt;strong&gt;3 times&lt;/strong&gt; that compute.
We already saw that the Loss function is &lt;strong&gt;huge&lt;/strong&gt;, it depends from every single element in your &lt;strong&gt;Training Set&lt;/strong&gt;
and we want to train our model on as many data as possible, because in practice more training datas means better model.
&lt;strong&gt;Gradient Descent&lt;/strong&gt; is also an iterative process, you have to do that for a lot of steps; that means going through your data tens or even &lt;strong&gt;thousands&lt;/strong&gt; of time, that&amp;rsquo;s no good.&lt;/p&gt;

&lt;p&gt;So we are going to cheat and instead of computing the actual Loss, we are going to compute an &lt;strong&gt;estimate&lt;/strong&gt; of it.
A &lt;strong&gt;very bad&lt;/strong&gt; estimate. A &lt;strong&gt;terrible&lt;/strong&gt; one. Damn, it&amp;rsquo;s so bad you will be wondering how it does even work.&lt;/p&gt;

&lt;p&gt;The estimate we are going to use is simply compute the &lt;strong&gt;Training Loss&lt;/strong&gt; for a &lt;strong&gt;very small random fraction&lt;/strong&gt; of our data.
We are taking between one and a thousand training samples each time.
It&amp;rsquo;s very important that the samples are completely random; if they are not, this is not going to work at all.&lt;/p&gt;

&lt;p&gt;The derivatives we will compute will not be the right direction at all, at times it might even &lt;strong&gt;increase&lt;/strong&gt; the Loss rather then reducing it.
But we are going to compensate by doing this many many times, taking small steps each time.&lt;/p&gt;

&lt;p&gt;Doing this is vastly more efficient than doing traditional Gradient Descent.&lt;/p&gt;

&lt;p&gt;This technique is called &lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt; (S.G.D.), and it&amp;rsquo;s at the core of &lt;strong&gt;Deep Learning&lt;/strong&gt; because it scales well with data and model size.&lt;/p&gt;

&lt;p&gt;SGD is the only optimizer fast enough, but it comes at a high price: because it&amp;rsquo;s a really bad estimation, we will need to spend some time optimizing it.&lt;/p&gt;

&lt;h3 id=&#34;momentum-and-learning-rate&#34;&gt;Momentum and Learning Rate&lt;/h3&gt;

&lt;p&gt;We have already seen some of this tricks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Inputs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;0 mean&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Equal variance&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Weights:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Random&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;0 mean&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Equal variance&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are a couple more tricks you can use to help SGD.&lt;/p&gt;

&lt;p&gt;The first one is momentum: remember that at each step we are taking a small step in a (random) direction, but that on aggragate those steps take us to the minimum of the Loss.
We can take advantage of the &lt;strong&gt;knowledge&lt;/strong&gt; we have accumulated from previous steps about where we should be heading.
A cheap way to do that is to keep a &lt;strong&gt;Running Average&lt;/strong&gt; of the gradients and to use that running average instead of the direction of the current batch of the data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://asolda.github.io/images/formulas/8.png&#34; style=&#34;margin: 0 auto; display: block; width:33%&#34;&gt;&lt;/p&gt;

&lt;p&gt;This &lt;strong&gt;Momentum&lt;/strong&gt; technique works very well and often lead to better convergence.&lt;/p&gt;

&lt;p&gt;The second one is &lt;strong&gt;Learning Rate Decay&lt;/strong&gt;: when replacing &lt;strong&gt;Gradient Descent&lt;/strong&gt; with &lt;strong&gt;SGD&lt;/strong&gt;, we realized we were going to take smaller and noisier steps towards our objective.
&lt;strong&gt;How small&lt;/strong&gt; should that step be? There&amp;rsquo;s not a correct answer for every case;
one thing that&amp;rsquo;s always the case however, is that is beneficial to make the steps smaller and smaller as we train.
Some like to apply an &lt;strong&gt;exponential decay&lt;/strong&gt; to their learning rate; others make it smaller when the Learning Rate hits a &lt;strong&gt;plateau&lt;/strong&gt;.
There are many ways to go about it, but &lt;strong&gt;lowering it over time&lt;/strong&gt; is the key thing to remember.&lt;/p&gt;

&lt;h3 id=&#34;too-many-parameters&#34;&gt;Too many parameters!!&lt;/h3&gt;

&lt;p&gt;SGD, a.k.a. &lt;strong&gt;Black Magic&lt;/strong&gt;. No wonder that&amp;rsquo;s its reputation. We have a lot of different parameters to tune:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Initial Learning Rate&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Learning Rate Decay&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Momentum&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Batch Size&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Weight Initialization&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In practice, it&amp;rsquo;s not that bad: your mantra will be &lt;em&gt;Keep calm, and lower your Learning Rate&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;One good approach is &lt;a href=&#34;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&#34;&gt;ADAGRAD&lt;/a&gt;, that can make things a little bit easier:
&lt;strong&gt;ADAGRAD&lt;/strong&gt; is a modification of SGD that takes care of &lt;strong&gt;Momentum&lt;/strong&gt; and &lt;strong&gt;Learning Rate&lt;/strong&gt; parameters for you.&lt;/p&gt;

&lt;p&gt;One final recap before moving on: we have a very simple linear model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://asolda.github.io/images/formulas/9.png&#34; style=&#34;margin: 0 auto; display: block; width:100%&#34;&gt;&lt;/p&gt;

&lt;p&gt;and we have learned how to optimize its parameters on lots and lots of data using &lt;strong&gt;SGD&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s still a &lt;strong&gt;linear&lt;/strong&gt; model, but now we have all the tools that we need: time to go deeper!&lt;/p&gt;

&lt;p&gt;In the next part we will introduce non linearity to our model and we will make it grow deeper and deeper. See you soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to Deep Learning: part 1</title>
      <link>https://asolda.github.io/post/deeplearning-1/</link>
      <pubDate>Wed, 07 Dec 2016 16:00:27 +0100</pubDate>
      
      <guid>https://asolda.github.io/post/deeplearning-1/</guid>
      <description>

&lt;h1 id=&#34;a-hopefully-useful-article&#34;&gt;A hopefully useful article&lt;/h1&gt;

&lt;p&gt;Was about time! This article is meant to be an &lt;strong&gt;introduction to Deep Learning&lt;/strong&gt; for newbies.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt; is a complex topic and often articles and blog posts are meant for people with a base knowledge about such topis;
This article instead is meant to be an entry point for people who are interested in learn new concepts and to get closer to this subject.&lt;/p&gt;

&lt;h2 id=&#34;from-machine-learning-to-deep-learning&#34;&gt;From Machine Learning to Deep Learning&lt;/h2&gt;

&lt;p&gt;Deep Learning is a branch of machine learning that uses data (&lt;strong&gt;lots&lt;/strong&gt; of data) to teach computers how to do things only humans were capable of before.&lt;/p&gt;

&lt;p&gt;A great example is the problem of perception, recognizing what&amp;rsquo;s in an image, what people are saying when they talk on their phones, helping robots explore the world around them.
Deep Learning is the state of the art tool to solve perception problem, and there&amp;rsquo;s more to it:
people are discovering that Deep Learning is extremely useful to solve problems like discovering new medicines, uderstanding natural language, understanding documents (and for example ranking them).&lt;/p&gt;

&lt;p&gt;Many big companies are using Deep Learning in their products and it&amp;rsquo;s easy to uderstand why: Deep Learning shines wherever thare is lots of data and complex problems to solve, and this companies need to solve some of the most complex problems out there (sounds like a cool job right?).&lt;/p&gt;

&lt;p&gt;A lot of the important work on neural networks happened in the 80&amp;rsquo;s and in the 90&amp;rsquo;s but back then computers were slow and datasets very tiny.
The research didn&amp;rsquo;t really find many applications in the real world, and as a result, in the first decade of the 21st century neural networks have completely disappeared from the world of machine learning.
It&amp;rsquo;s only in the last few years that neural networks made a big comeback. What changed? &lt;strong&gt;Lots of data&lt;/strong&gt; and &lt;strong&gt;cheap and fast GPU&amp;rsquo;s&lt;/strong&gt;.
Today, neural networks are &lt;strong&gt;everywhere&lt;/strong&gt;.
So if you&amp;rsquo;re doing anything with &lt;strong&gt;data&lt;/strong&gt;, &lt;strong&gt;analytics&lt;/strong&gt; or &lt;strong&gt;prediction&lt;/strong&gt;, they&amp;rsquo;re definitely something that you want to get familiar with.&lt;/p&gt;

&lt;p&gt;For this article we will focus on the problem of &lt;strong&gt;classification&lt;/strong&gt;: classification is the task of taking an input (say, a photo of a handwritten letter) and assign it a label (the actual letter).
Tipically you have a lot of &lt;strong&gt;examples&lt;/strong&gt;, called &amp;ldquo;&lt;strong&gt;Training Set&lt;/strong&gt;&amp;rdquo;, that have already been solved; then you take a complete new example and try to figure out which class it belongs.&lt;/p&gt;

&lt;p&gt;There is much more to Machine Learning than just classification, but once you learn how to classify objects you can build up to amazing things.
Our aim today is to build a &lt;strong&gt;Logistic classifier&lt;/strong&gt; before moving on to actual Deep Learning.&lt;/p&gt;

&lt;h3 id=&#34;logistic-classifier&#34;&gt;Logistic classifier&lt;/h3&gt;

&lt;p&gt;.A Logistic Classifier is what is called a &lt;strong&gt;Linear Classifier&lt;/strong&gt;: it takes the inputs and applyes a linear function to them to generate its prediction.
The mathematical formulation is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://asolda.github.io/images/formulas/1.png&#34; style=&#34;margin: 0 auto; display: block; width:25%&#34;&gt;&lt;/p&gt;

&lt;p&gt;where the &lt;em&gt;X&lt;/em&gt; variable represents the input (the pixels of an image maybe) and &lt;em&gt;y&lt;/em&gt; is the prediction;
the linear function is just a giant matrix multiplication: it takes the input as a vector and multiplies it with the &lt;strong&gt;weigth matrix&lt;/strong&gt;.
The other vector in the equation, &lt;em&gt;b&lt;/em&gt;, is called the &lt;strong&gt;bias vector&lt;/strong&gt;; more on that later.
&lt;strong&gt;Weigths&lt;/strong&gt; and &lt;strong&gt;bias&lt;/strong&gt; are where the Machine Learning kicks in: we are going to train the model, meaning that we are looking for the values of &lt;em&gt;W&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; which are &amp;ldquo;good&amp;rdquo; at performing predictions.&lt;/p&gt;

&lt;p&gt;The output will contain &lt;strong&gt;scores&lt;/strong&gt; for each possible class; if the score for a class is higher, then it&amp;rsquo;s likely that the input belongs to that class.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://asolda.github.io/images/formulas/2.png&#34; style=&#34;margin: 0 auto; display: block; heigth:25%&#34;&gt;&lt;/p&gt;

&lt;p&gt;However, since an input can only belong to a single class, we need to turn these scores into probabilties.
In order to do use, we can use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Softmax_function&#34;&gt;Softmax function&lt;/a&gt;.
What&amp;rsquo;s important to know is that it takes the scores and turn them into probabilities.&lt;/p&gt;

&lt;p&gt;After we apply the Softmax function, we find out that the probability for the right class is higher, and other probabilities are much smaller:
this help us to represent the outputs in a more practical way using the &lt;strong&gt;One-Hot Encoding&lt;/strong&gt;: we create a new vector where the right class has a value of 1, and all the other values are 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://asolda.github.io/images/formulas/3.png&#34; style=&#34;margin: 0 auto; display: block; heigth:25%&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Small recap&lt;/strong&gt;: we start from our input (as a vector), we apply the linear function to obtain scores for each class; we then convert scores to probabilities with a Softmax function and finally we apply one-hot encoding to the resulting vector.&lt;/p&gt;

&lt;p&gt;One-hot encoding works very well with a small number of classes, but whan we have tens of thousands of classes what we get are large vectors filled with 0; not ideal right?
We can deal with this problem using &lt;strong&gt;embeddings&lt;/strong&gt;, we will eventually get there.&lt;/p&gt;

&lt;p&gt;The good thing about this approach is that we can now mesure how well our model is performing by comparing the output vector with the probabilities and the one-hot vector corresponding to your labels.
The natural way to measure the distance between two probability vectors is called the &lt;strong&gt;Cross Entropy&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://asolda.github.io/images/formulas/4.png&#34; style=&#34;margin: 0 auto; display: block; width:37%&#34;&gt;&lt;/p&gt;

&lt;p&gt;Remember that &lt;em&gt;S&lt;/em&gt; is the vector with our probabilities and &lt;em&gt;L&lt;/em&gt; is the one-hot encoded vector.&lt;/p&gt;

&lt;p&gt;This entire settings is often called &lt;strong&gt;Multinomial Logistic Classification&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Now we have all the pieces and the question is: how are we going to find the values for &lt;em&gt;W&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt;?
We want values such that the distance for the correct class is low, but the distances for the wrong classes are high.&lt;/p&gt;

&lt;p&gt;One thing we can do, is measure this distance averaged over the entire training set:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://asolda.github.io/images/formulas/5.png&#34; style=&#34;margin: 0 auto; display: block; width:37%&#34;&gt;&lt;/p&gt;

&lt;p&gt;This is called the &lt;strong&gt;training loss&lt;/strong&gt;; this is a big sum with a large matrix in it, so be careful&amp;hellip;&lt;/p&gt;

&lt;p&gt;We want all distances to be small, which means that we are soing a good job at classifying data, so we want out loss to be small.
The loss is a function of &lt;em&gt;W&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt;, so we are simply trying to minimize this function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We have just turned the Machine Learning problem into one of numerical optimization&lt;/strong&gt;. You&amp;rsquo;re welcome.&lt;/p&gt;

&lt;p&gt;There are many ways to solve a numerical optimization problems; a famous one is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;Gradient Descent&lt;/a&gt;:
take the derivative of your loss with respect to the parameters and &amp;ldquo;follow&amp;rdquo; that derivative; repeat until you get to the bottom.&lt;/p&gt;

&lt;p&gt;We need to make a little pause here and talk about &lt;strong&gt;numerical stability&lt;/strong&gt; and &lt;strong&gt;conditioning&lt;/strong&gt;; I will not talk deeply about these aspects, just keep in mind that we want our values to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Have 0 &lt;strong&gt;mean&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Have equal &lt;strong&gt;variance&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is important to improve the performances of our classifier.
A small &lt;strong&gt;practical examples&lt;/strong&gt; with images: take the pixel values of your image (tipically between 0 and 255) and subtract 128; then divide by 128.
This doesn&amp;rsquo;t change the content of your image, but it makes much easier to the optimization to proceed numerically.&lt;/p&gt;

&lt;p&gt;Another important factor is how you &lt;strong&gt;initialize&lt;/strong&gt; your weigths and biases.
An easy and general approach is to initialize the weigths randomly with a &lt;a href=&#34;http://hyperphysics.phy-astr.gsu.edu/hbase/Math/gaufcn.html&#34;&gt;Gaussian distribution&lt;/a&gt; of mean 0 and standard deviation &lt;em&gt;sigma&lt;/em&gt;.
The &lt;em&gt;sigma&lt;/em&gt; value determines the order of magnitude of your outputs at the initial point of your optimization.
Because of the soft max on top of it, the order of magnitude also determines the peakiness of your initial probability distribution.
A &lt;strong&gt;large&lt;/strong&gt; sigma means that your distribution will have large peaks, so it&amp;rsquo;s going to be very opinionated.
A &lt;strong&gt;small&lt;/strong&gt; sigma means that your distribution is very uncertain about things.
It&amp;rsquo;s usually better to begin with an &lt;strong&gt;uncertain&lt;/strong&gt; distribution and let the optimization become more confident as the train progress.
So we will use a small sigma to begin with.&lt;/p&gt;

&lt;p&gt;The bias vector can be initialized with 0.&lt;/p&gt;

&lt;p&gt;We now have everything we need to build a working classifier; let&amp;rsquo;s stick with the images example:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Process&lt;/strong&gt; images: for each pixel, the value &lt;em&gt;v&lt;/em&gt; become (v - 128)/128&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Multiply&lt;/strong&gt; your input and the matrix &lt;em&gt;W&lt;/em&gt; and add the biases&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Apply &lt;strong&gt;Softmax&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Compute the &lt;strong&gt;Cross-Entropy loss&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Calculate tha average of the &lt;strong&gt;Training Loss&lt;/strong&gt; over the entire training data&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Optimization&lt;/strong&gt; process:
&lt;img src=&#34;https://asolda.github.io/images/formulas/6.png&#34; style=&#34;margin: 0 auto; display: block; width:37%&#34;&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat from step 2&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;end-of-part-one&#34;&gt;End of part one&lt;/h2&gt;

&lt;p&gt;Believe it or not, we have just gone over the essentials things that you need to train a &lt;strong&gt;Linear Classifier&lt;/strong&gt;.
This works pretty well, but &lt;strong&gt;it can be very slow&lt;/strong&gt;, even for small datasets and for simple models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scaling up&lt;/strong&gt; this model is what &lt;strong&gt;Deep Learning&lt;/strong&gt; is all about.&lt;/p&gt;

&lt;p&gt;In this article we have understood what are the mathematical foundations for Machine Learning;
this is an important step to fully understand what is Deep Learning and, more importantly, what are the problems that you can solve with Deep Learning tecniques.&lt;/p&gt;

&lt;p&gt;In the next part we will understand the differences between Machine and Deep learning and why these models are different.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bake your own Deep Dream</title>
      <link>https://asolda.github.io/post/deepdream/</link>
      <pubDate>Tue, 06 Dec 2016 14:56:57 +0100</pubDate>
      
      <guid>https://asolda.github.io/post/deepdream/</guid>
      <description>

&lt;p&gt;This article is a follow up to &lt;a href=&#34;https://asolda.github.io/post/deepdream/&#34;&gt;this article&lt;/a&gt;; I suggest you check it out before moving on.&lt;/p&gt;

&lt;h1 id=&#34;an-article-with-a-purpose&#34;&gt;An article with a purpose&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt; is, without a doubt, an interesting and fascinating topic.
It can be applied in many different contexts, from image recognition to text processing;
companies like Google and Facebook heavily rely on Deep Learning to offer many of their services.&lt;/p&gt;

&lt;p&gt;However, we are not going to understand how DL can integrate inside business logics; that would be constructive and useful, &lt;em&gt;ergo&lt;/em&gt; not suited for this blog.&lt;/p&gt;

&lt;p&gt;Instead, we are going to take a deeper look at Google &lt;a href=&#34;https://en.wikipedia.org/wiki/DeepDream&#34;&gt;Deep Dream&lt;/a&gt;,
aiming to create this hypnotic images by ourself.
Please note that, jokes aside, this &lt;strong&gt;is&lt;/strong&gt; indeed useful, allowing us to better undestand the processes that take place during the classification procedure.&lt;/p&gt;

&lt;h2 id=&#34;tools&#34;&gt;Tools&lt;/h2&gt;

&lt;p&gt;Once again, we are going to use &lt;a href=&#34;https://keras.io/&#34;&gt;Keras&lt;/a&gt; on top of &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt;,
to mantain the code readable and to avoid complications.&lt;/p&gt;

&lt;p&gt;We are going to implement our own Deep Dream convnet using the &lt;a href=&#34;https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3&#34;&gt;pre-trained weights&lt;/a&gt; we have already used last time.&lt;/p&gt;

&lt;p&gt;The code, however, will be slightly different and we are not reusing the one we wrote last time.&lt;/p&gt;

&lt;h1 id=&#34;the-coding-bit&#34;&gt;The coding bit&lt;/h1&gt;

&lt;p&gt;First of, we start with a few utilities:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from __future__ import print_function
from keras.preprocessing.image import load_img, img_to_array
import numpy as np
from scipy.misc import imsave
from scipy.optimize import fmin_l_bfgs_b
import time
import argparse

from keras.applications import vgg16
from keras import backend as K
from keras.layers import Input

parser = argparse.ArgumentParser(description=&#39;Deep dream implelentation with TensorFlow&#39;)
parser.add_argument(&#39;base_image_path&#39;, metavar=&#39;base&#39;, type=str,
                    help=&#39;Path to the image to transform.&#39;)
parser.add_argument(&#39;result_prefix&#39;, metavar=&#39;res_prefix&#39;, type=str,
                    help=&#39;Prefix for the saved results.&#39;)

args = parser.parse_args()
base_image_path = args.base_image_path
result_prefix = args.result_prefix

N_ITER = 100

# dimensions of the generated picture.
img_width = 450
img_height = 900

# path to the model weights file.
weights_path = &#39;vgg16_weights.h5&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These utilities include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;import&lt;/code&gt; statements&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Argument&lt;/strong&gt; handling: we expect two parameters, the starting image and a prefix to save the output images&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The maximum number of iterations to perform (&lt;code&gt;N_ITER&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dimensions&lt;/strong&gt; for the generated images&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;weights&lt;/strong&gt; to use for our network (VGG16)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Loading the model is, like last time, pretty straightforward:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# this will contain our generated image
dream = Input(batch_shape=(1,) + img_size)

# build the VGG16 network with our placeholder
# the model will be loaded with pre-trained ImageNet weights
model = vgg16.VGG16(input_tensor=dream,
                    weights=&#39;imagenet&#39;, include_top=False)
print(&#39;Model loaded.&#39;)

layer_dict = dict([(layer.name, layer) for layer in model.layers])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We create a &lt;code&gt;placeholder&lt;/code&gt; and we use it as the &lt;code&gt;input_tensor&lt;/code&gt; for our network.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;img_size&lt;/code&gt; variable is computed this way:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if K.image_dim_ordering() == &#39;th&#39;:
    img_size = (3, img_width, img_height)
else:
    img_size = (img_width, img_height, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;dim_ordering&lt;/code&gt; can either be &amp;ldquo;tf&amp;rdquo; or &amp;ldquo;th&amp;rdquo;.
It tells Keras whether to use Theano or TensorFlow dimension ordering for inputs/kernels/ouputs.&lt;/p&gt;

&lt;p&gt;We can now write our loss function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# define the loss
loss = K.variable(0.)
for layer_name in settings[&#39;features&#39;]:
    # add the L2 norm of the features of a layer to the loss
    assert layer_name in layer_dict.keys(), &#39;Layer &#39; + layer_name + &#39; not found in model.&#39;
    coeff = settings[&#39;features&#39;][layer_name]
    x = layer_dict[layer_name].output
    shape = layer_dict[layer_name].output_shape
    # we avoid border artifacts by only involving non-border pixels in the loss
    if K.image_dim_ordering() == &#39;th&#39;:
        loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2] - 2, 2: shape[3] - 2])) / np.prod(shape[1:])
    else:
        loss -= coeff * K.sum(K.square(x[:, 2: shape[1] - 2, 2: shape[2] - 2, :])) / np.prod(shape[1:])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next step is applying a couple of tweaks to achieve better results:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;continuity loss&lt;/strong&gt;, to give the image local coherence and avoid messy blurs&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;strong&gt;L2 norm loss&lt;/strong&gt; in order to prevent pixels from taking very high values&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two lines of code will suffice:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# add continuity loss
loss += settings[&#39;continuity&#39;] * continuity_loss(dream) / np.prod(img_size)
# add image L2 norm to loss
loss += settings[&#39;dream_l2&#39;] * K.sum(K.square(dream)) / np.prod(img_size)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;continuity_loss&lt;/code&gt; is a utility function defined by us:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# continuity loss util function
def continuity_loss(x):
    assert K.ndim(x) == 4
    if K.image_dim_ordering() == &#39;th&#39;:
        a = K.square(x[:, :, :img_width - 1, :img_height - 1] -
                     x[:, :, 1:, :img_height - 1])
        b = K.square(x[:, :, :img_width - 1, :img_height - 1] -
                     x[:, :, :img_width - 1, 1:])
    else:
        a = K.square(x[:, :img_width - 1, :img_height-1, :] -
                     x[:, 1:, :img_height - 1, :])
        b = K.square(x[:, :img_width - 1, :img_height-1, :] -
                     x[:, :img_width - 1, 1:, :])
    return K.sum(K.pow(a + b, 1.25))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our model is now complete.
You can feel free to further modify the loss as you see fit, to achieve new effects.&lt;/p&gt;

&lt;p&gt;Now things can get a little bit trickier:
we need to evaluate our loss and our gradients in one pass, but &lt;code&gt;scipy.optimize&lt;/code&gt; requires separate functions for loss and gradients,
and computing them separately would be inefficient.
To solve this we create our own &lt;code&gt;Evaluator&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Evaluator(object):
    def __init__(self):
        self.loss_value = None
        self.grad_values = None

    def loss(self, x):
        assert self.loss_value is None
        loss_value, grad_values = eval_loss_and_grads(x)
        self.loss_value = loss_value
        self.grad_values = grad_values
        return self.loss_value

    def grads(self, x):
        assert self.loss_value is not None
        grad_values = np.copy(self.grad_values)
        self.loss_value = None
        self.grad_values = None
        return grad_values

evaluator = Evaluator()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;eval_loss_and_grads&lt;/code&gt; functions will then read as follow:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# compute the gradients of the dream wrt the loss
grads = K.gradients(loss, dream)

outputs = [loss]
if type(grads) in {list, tuple}:
    outputs += grads
else:
    outputs.append(grads)

f_outputs = K.function([dream], outputs)
def eval_loss_and_grads(x):
    x = x.reshape((1,) + img_size)
    outs = f_outputs([x])
    loss_value = outs[0]
    if len(outs[1:]) == 1:
        grad_values = outs[1].flatten().astype(&#39;float64&#39;)
    else:
        grad_values = np.array(outs[1:]).flatten().astype(&#39;float64&#39;)
    return loss_value, grad_values
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All that&amp;rsquo;s left now is to run &lt;a href=&#34;https://en.wikipedia.org/wiki/Limited-memory_BFGS&#34;&gt;L-BFGS&lt;/a&gt; optimizer over the pixels of the generated image,
in order to minimize the loss:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = preprocess_image(base_image_path)
for i in range(N_ITER):
    print(&#39;Start of iteration&#39;, i)
    start_time = time.time()

    # add a random jitter to the initial image. This will be reverted at decoding time
    random_jitter = (settings[&#39;jitter&#39;] * 2) * (np.random.random(img_size) - 0.5)
    x += random_jitter

    # run L-BFGS for 7 steps
    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),
                                     fprime=evaluator.grads, maxfun=7)
    print(&#39;Current loss value:&#39;, min_val)
    # decode the dream and save it
    x = x.reshape(img_size)
    x -= random_jitter
    img = deprocess_image(np.copy(x))
    fname = result_prefix + &#39;_at_iteration_%d.png&#39; % i
    imsave(fname, img)
    end_time = time.time()
    print(&#39;Image saved as&#39;, fname)
    print(&#39;Iteration %d completed in %ds&#39; % (i, end_time - start_time))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What are we missing? Just a couple of utility functions and a custom configuration (the &lt;code&gt;settings&lt;/code&gt; variable) for our network:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Here are the functions &lt;code&gt;preprocess_image&lt;/code&gt; and &lt;code&gt;deprocess_image&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;# util function to open, resize and format pictures into appropriate tensors
def preprocess_image(image_path):
    img = load_img(image_path, target_size=(img_width, img_height))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg16.preprocess_input(img)
    return img

# util function to convert a tensor into a valid image
def deprocess_image(x):
    if K.image_dim_ordering() == &#39;th&#39;:
        x = x.reshape((3, img_width, img_height))
        x = x.transpose((1, 2, 0))
    else:
        x = x.reshape((img_width, img_height, 3))
    # Remove zero-center by mean pixel
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68
    # &#39;BGR&#39;-&amp;gt;&#39;RGB&#39;
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype(&#39;uint8&#39;)
    return x
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Next up, a couple of example settings that I like:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;saved_settings = {
    &#39;acid&#39;: {&#39;features&#39;: {&#39;block4_conv1&#39;: 0.05,
                              &#39;block4_conv2&#39;: 0.01,
                              &#39;block4_conv3&#39;: 0.01},
                 &#39;continuity&#39;: 0.1,
                 &#39;dream_l2&#39;: 0.8,
                 &#39;jitter&#39;: 5},
    &#39;doggos&#39;: {&#39;features&#39;: {&#39;block5_conv1&#39;: 0.05,
                            &#39;block5_conv2&#39;: 0.02},
               &#39;continuity&#39;: 0.1,
               &#39;dream_l2&#39;: 0.02,
               &#39;jitter&#39;: 0},
}

# the settings we will use in this experiment
settings = saved_settings[&#39;doggos&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The results that I&amp;rsquo;ll show below are obtained with the &lt;code&gt;doggos&lt;/code&gt; setting.&lt;/p&gt;

&lt;h1 id=&#34;the-fun-part&#34;&gt;The fun part&lt;/h1&gt;

&lt;p&gt;To test our model I&amp;rsquo;ve chosen famous paintings and I&amp;rsquo;ve run the code above for an &amp;ldquo;appropriate&amp;rdquo; number of iterations.
I usually prefer images where you can recognize both the original painting and the network&amp;rsquo;s work.&lt;/p&gt;

&lt;h2 id=&#34;input-1-nascita-di-venere&#34;&gt;Input 1: Nascita di Venere&lt;/h2&gt;

&lt;p&gt;The &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/The_Birth_of_Venus&#34;&gt;Birth if Venus&lt;/a&gt;&lt;/em&gt; is a painting by Sandro Botticelli generally thought to have been painted in the mid 1480s.
This is an iconic and easily recognizable painting:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/venus.jpg&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/venus.jpg&#34; style=&#34;margin: 0 auto; display: block; width:75%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are the results after various number of iterations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After &lt;strong&gt;1&lt;/strong&gt; iteration&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/venus-1.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/venus-1.png&#34; style=&#34;margin: 0 auto; display: block; width:60%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The painting is clearly there, almost untouched, but you can already see that weird images are forming.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After &lt;strong&gt;5&lt;/strong&gt; iterations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/venus-5.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/venus-5.png&#34; style=&#34;margin: 0 auto; display: block; width:60%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After &lt;strong&gt;10&lt;/strong&gt; iterations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/venus-10.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/venus-10.png&#34; style=&#34;margin: 0 auto; display: block; width:60%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here the painting has been heavily modified, and you can recognize animals that the network was trained to recognize.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After &lt;strong&gt;20&lt;/strong&gt; iterations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/venus-20.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/venus-20.png&#34; style=&#34;margin: 0 auto; display: block; width:60%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;And finally, after &lt;strong&gt;25&lt;/strong&gt; iterations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/venus-25.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/venus-25.png&#34; style=&#34;margin: 0 auto; display: block; width:75%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is the point where I like it the most, but it&amp;rsquo;s a matter of personal test and you can perform as many iterations as you like.&lt;/p&gt;

&lt;h2 id=&#34;input-2-creazione-di-adamo&#34;&gt;Input 2: Creazione di Adamo&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Creation_of_Adam&#34;&gt;Creation of Adam&lt;/a&gt; is a fresco painting by Michelangelo, which forms part of the Sistine Chapel&amp;rsquo;s ceiling, painted c. 1508â€“1512.
Yet another very famous panting:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/adam.jpg&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/adam.jpg&#34; style=&#34;margin: 0 auto; display: block; width:75%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how our network will deface this painting:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After &lt;strong&gt;1&lt;/strong&gt; iteration:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/adam-1.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/adam-1.png&#34; style=&#34;margin: 0 auto; display: block; width:60%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After &lt;strong&gt;20&lt;/strong&gt; iterations:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/adam-20.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/adam-20.png&#34; style=&#34;margin: 0 auto; display: block; width:60%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After &lt;strong&gt;40&lt;/strong&gt; iterations:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/adam-40.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/adam-40.png&#34; style=&#34;margin: 0 auto; display: block; width:60%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After &lt;strong&gt;60&lt;/strong&gt; iterations:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/adam-60.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/adam-60.png&#34; style=&#34;margin: 0 auto; display: block; width:75%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;other-tests&#34;&gt;Other tests&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve tested the code on many different paintings and photos, I will just include here the two that I liked the most:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/sunflowers.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/sunflowers.png&#34; style=&#34;margin: 0 auto; display: block; width:33%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/river.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/river.png&#34; style=&#34;margin: 0 auto; display: block; width:60%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;My bet is you will recognize this paintings.&lt;/p&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Deep Dream help us understand and visualize how neural networks are able to carry out difficult classification tasks, improve network architecture, and check what the network has learned during training.
It also makes us wonder whether neural networks could become a tool for artists, a new way to remix visual concepts, or perhaps even shed a little light on the roots of the creative process in general.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The day I found out my computer does drugs</title>
      <link>https://asolda.github.io/post/convnet/</link>
      <pubDate>Mon, 05 Dec 2016 14:42:46 +0100</pubDate>
      
      <guid>https://asolda.github.io/post/convnet/</guid>
      <description>

&lt;h1 id=&#34;exploring-the-unknown&#34;&gt;Exploring the unknown&lt;/h1&gt;

&lt;p&gt;In this article we will talk about &lt;strong&gt;deep convolutional neural network&lt;/strong&gt;, a.k.a. how computers are going to rule the world.
Someday. Eventually. Definetely not right now, mostly because I&amp;rsquo;ve come to the realization that my computer is constantly high on some weird acid.
More on this later. For now, let&amp;rsquo;s just understand that we are going to talk about complex topics, that this article isn&amp;rsquo;t meant for everyone out there and that I won&amp;rsquo;t have the time to explain everithing I say&amp;hellip;
I just hope you&amp;rsquo;ll follow through.&lt;/p&gt;

&lt;h2 id=&#34;what-are-we-doing&#34;&gt;What are we doing?&lt;/h2&gt;

&lt;p&gt;We want to take a look at what deep convolutional neural networks (convnets) really learn, and how they understand the images we feed them.&lt;/p&gt;

&lt;p&gt;We will use &lt;a href=&#34;https://keras.io/&#34;&gt;Keras&lt;/a&gt;, a Deep Learning library for &lt;a href=&#34;http://deeplearning.net/software/theano/&#34;&gt;Theano&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Keras&lt;/strong&gt; is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano.
It was developed with a focus on enabling fast experimentation, allowing researchers to &amp;ldquo;&lt;em&gt;go from idea to result with the least possible delay&lt;/em&gt;&amp;rdquo;.
It is a really interesting project and can be extremely useful.
In this post I will be using &lt;strong&gt;Keras&lt;/strong&gt; on top of &lt;strong&gt;TensorFlow&lt;/strong&gt;, but the code can be seamlessy adapted to work on top of Theano if you prefer.&lt;/p&gt;

&lt;p&gt;We will use Keras to visualize inputs that maximize the activation of the filters in different layers of the VGG16 architecture, trained on &lt;a href=&#34;http://www.image-net.org/&#34;&gt;ImageNet&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;visualizing-convnet-filters&#34;&gt;Visualizing convnet filters&lt;/h1&gt;

&lt;p&gt;VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the &lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/&#34;&gt;Visual Geometry Group&lt;/a&gt; from Oxford, who developed it.
It was used to win the &lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/research/very_deep/&#34;&gt;ILSVR (ImageNet) competition in 2014&lt;/a&gt;.
To this day is it still considered to be an excellent vision model, although it has been somewhat outperformed by more recent advances (Inception, ResNet).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.lorenzobaraldi.com/&#34;&gt;Lorenzo Baraldi&lt;/a&gt; ported the pre-trained Caffe version of VGG16 (as well as VGG19) to a Keras weights file, so we will just load that to do our experiments. You can download the weight file &lt;a href=&#34;https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Loading the model is made easy by Keras:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# build the VGG16 network with ImageNet weights
model = vgg16.VGG16(weights=&#39;imagenet&#39;, include_top=False)
print(&#39;Model loaded.&#39;)

model.summary()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yup, it&amp;rsquo;s &lt;strong&gt;that&lt;/strong&gt; easy. Moving along, we prepare a couple of utilities that we will need later:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;placeholder&lt;/strong&gt; (&lt;a href=&#34;https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops.html&#34;&gt;read the doc&lt;/a&gt; if you&amp;rsquo;re confused):&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;# this is the placeholder for the input images
input_img = model.input
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;dictionary&lt;/strong&gt; that we will use to create &lt;em&gt;ad-hoc&lt;/em&gt; loss functions:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;# get the symbolic outputs of each &amp;quot;key&amp;quot; layer (we gave them unique names).
layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;A utility function to &lt;strong&gt;normalize&lt;/strong&gt; a Tensor (using its L2 Norm)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;def normalize(x):
    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Moving to the interesting part of the code, it&amp;rsquo;s now time to scan through the filters, compute the loss functions and visualize the most interesting filters.
The scanning process is pretty straightforward:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kept_filters = []
for filter_index in range(0, 200):
    # we only scan through the first 200 filters,
    # but there are more (512)
    print(&#39;Processing filter %d&#39; % filter_index)
    start_time = time.time()

    # we build a loss function that maximizes the activation
    # of the nth filter of the layer considered
    layer_output = layer_dict[layer_name].output
    if K.image_dim_ordering() == &#39;th&#39;:
        loss = K.mean(layer_output[:, filter_index, :, :])
    else:
        loss = K.mean(layer_output[:, :, :, filter_index])

    # we compute the gradient of the input picture wrt this loss
    grads = K.gradients(loss, input_img)[0]

    # normalization trick: we normalize the gradient
    grads = normalize(grads)

    # this function returns the loss and grads given the input picture
    iterate = K.function([input_img], [loss, grads])

    # step size for gradient ascent
    step = 1.

    # we start from a gray image with some random noise
    if K.image_dim_ordering() == &#39;th&#39;:
        input_img_data = np.random.random((1, 3, img_width, img_height))
    else:
        input_img_data = np.random.random((1, img_width, img_height, 3))
    input_img_data = (input_img_data - 0.5) * 20 + 128

    # we run gradient ascent for 20 steps
    for i in range(20):
        loss_value, grads_value = iterate([input_img_data])
        input_img_data += grads_value * step

        print(&#39;Current loss value:&#39;, loss_value)
        if loss_value &amp;lt;= 0.:
            # some filters get stuck to 0, we can skip them
            break

    # decode the resulting input image
    if loss_value &amp;gt; 0:
        img = deprocess_image(input_img_data[0])
        kept_filters.append((img, loss_value))
    end_time = time.time()
    print(&#39;Filter %d processed in %ds&#39; % (filter_index, end_time - start_time))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code is well commented and Keras makes everything extremely linear and readable.
Just a couple of notes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;loss function&lt;/code&gt; is easy, but effective for our purpose.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &amp;ldquo;&lt;em&gt;normalization trick&lt;/em&gt;&amp;rdquo; is worth pointing out, start your research from &lt;a href=&#34;http://stats.stackexchange.com/questions/22568/difference-in-using-normalized-gradient-and-gradient&#34;&gt;here&lt;/a&gt; if you are interested.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;code&gt;deprocess_image&lt;/code&gt; function can be a little bit tricky, but here you are:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;# util function to convert a tensor into a valid image
def deprocess_image(x):
    # normalize tensor: center on 0., ensure std is 0.1
    x -= x.mean()
    x /= (x.std() + 1e-5)
    x *= 0.1

    # clip to [0, 1]
    x += 0.5
    x = np.clip(x, 0, 1)

    # convert to RGB array
    x *= 255
    if K.image_dim_ordering() == &#39;th&#39;:
        x = x.transpose((1, 2, 0))
    x = np.clip(x, 0, 255).astype(&#39;uint8&#39;)
    return x
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;layer_name&lt;/code&gt; variable should look like this:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;# the name of the layer we want to visualize
# (see model definition at keras/applications/vgg16.py)
layer_name = &#39;block5_conv1&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now move on to select the &amp;ldquo;best&amp;rdquo; layers;
the filters that have the highest loss are supposed to be the best looking:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# we will stich the best 64 filters on a 8 x 8 grid.
n = 8

# we will only keep the top 64 filters.
kept_filters.sort(key=lambda x: x[1], reverse=True)
kept_filters = kept_filters[:n * n]

# build a black picture with enough space for
# our 8 x 8 filters of size 128 x 128, with a 5px margin in between
margin = 5
width = n * img_width + (n - 1) * margin
height = n * img_height + (n - 1) * margin
stitched_filters = np.zeros((width, height, 3))

# fill the picture with our saved filters
for i in range(n):
    for j in range(n):
        img, loss = kept_filters[i * n + j]
        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,
                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img

# save the result to disk
imsave(&#39;stitched_filters_%dx%d.png&#39; % (n, n), stitched_filters)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dimensions of the generated pictures for each filter.
img_width = 128
img_height = 128
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the image we are getting is this:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asolda.github.io/images/filters.png&#34;&gt;&lt;img src=&#34;https://asolda.github.io/images/filters.png&#34; style=&#34;margin: 0 auto; display: block; width:50%&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can obviously use this code to visualize different filters for each layer.
If you do this, you will notice that the first layers basically just encode direction and color.
These direction and color filters then get combined into basic grid and spot textures.
These textures gradually get combined into increasingly complex patterns.&lt;/p&gt;

&lt;p&gt;In the highest layers (like the one above) we start to recognize textures similar to that found in the objects that network was trained to classify, such as feathers, eyes, etc.&lt;/p&gt;

&lt;p&gt;Another fun thing to do is to apply these filters to photos (rather than to noisy all-gray inputs).
This is the principle of &lt;a href=&#34;https://en.wikipedia.org/wiki/DeepDream&#34;&gt;Deep Dreams&lt;/a&gt;, popularized by Google last year.&lt;/p&gt;

&lt;p&gt;We will learn how to implement Deep Dream in the next article, in the meanwhile you can have fun visualizing different layers and using photos as a starting point for filters visualization.&lt;/p&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;The key message here is that &lt;strong&gt;convnets don&amp;rsquo;t understand concepts&lt;/strong&gt; just because thay are able to classify objects.
The visual decomposition of visual space learned by a convnet is (apparently) analogous to what the human visual cortex does.
It may or may not be true. So what do they understand? Basically two things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;They understand a &lt;strong&gt;decomposition of their visual input space&lt;/strong&gt; as a hierarchical-modular network of convolution filters.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;They understand a &lt;strong&gt;probabilitistic mapping between certain combinations of these filters&lt;/strong&gt; and a set of arbitrary labels.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Naturally, this does not qualify as &amp;ldquo;seeing&amp;rdquo; in any human sense, and it doesn&amp;rsquo;t mean we have solved computer vision.&lt;/p&gt;

&lt;p&gt;That said, visualizing what convnets learn is quite fascinating.
Deep learning may not be intelligence in any real sense, but it&amp;rsquo;s still working considerably better than anybody could have anticipated just a few years ago.
Guess it&amp;rsquo;s about time someone figures out &lt;strong&gt;why&lt;/strong&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Hugo turned me into a blogger wannabe</title>
      <link>https://asolda.github.io/post/hugo-blogger/</link>
      <pubDate>Thu, 01 Dec 2016 15:13:03 +0100</pubDate>
      
      <guid>https://asolda.github.io/post/hugo-blogger/</guid>
      <description>

&lt;h1 id=&#34;what-is-this&#34;&gt;What is this?&lt;/h1&gt;

&lt;p&gt;I have decided to finally start a blog. This is just a welcome message (mostly realized to test Hugo, Github Pages and a few themes).
In this blog I will talk about the project I work on in my free time; topics range from machine/deep learning to social network analysis, distributed computing and more.
Some projects are my own ideas, others are the result of me playing with coll stuff you find around the web.
This first article is about the technical details behind the blog itself, what technologies I&amp;rsquo;m using, how I&amp;rsquo;ve set up my workflow and how hugo convinced me to start this.&lt;/p&gt;

&lt;h1 id=&#34;about-this-article&#34;&gt;About this article&lt;/h1&gt;

&lt;p&gt;This is supposed to be a technical blog, a safe shelter for computer scientists and alikes.
How I am developing this blog can be interesting and usefull for other people who want to share their projects and their knowledge.
The idea here is to give you an overview of how I am developing and updating the blog from a technical perspective.&lt;/p&gt;

&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;, alongside &lt;a href=&#34;https://pages.github.com/&#34;&gt;Github Pages&lt;/a&gt;, are the main reasons why I have finally decided to start this project.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hugo&lt;/strong&gt; is &amp;ldquo;&lt;em&gt;a fast and modern static website engine&lt;/em&gt;&amp;rdquo;. That is, a &lt;strong&gt;HUGE&lt;/strong&gt; help when you want to just write your thoughs, without worrying about themes, styles, and general web-development-related issues.
It is easy to install and configure (short tutorial is down the road), light, fast (gotta love &lt;a href=&#34;https://golang.org/&#34;&gt;go&lt;/a&gt;), open source&amp;hellip; I could go on for miles.
There are downsides and negative aspects too, but the moment you realize what Hugo is meant for you also realize that negative aspects turn into features; you will realize this in a few weeks of usage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Github Pages&lt;/strong&gt; allows you to create and publish websites for your projects, as well as personal blogs or portfolios.
The code for you site is hosted directly from your GitHub repository. Just edit, push, and your changes are live.&lt;/p&gt;

&lt;p&gt;The way Hugo and Github Pages interact is mesmerizing; this instruments team up for a produtictivity boost comparable to the introductions of rounded wheels over squared ones.
Damn, you gotta love this things; and to bring you on board with me, let&amp;rsquo;s dive into a tutorial on how to setup this workflow.&lt;/p&gt;

&lt;p&gt;Please note that I will assume that you already use git. In case you don&amp;rsquo;t, &lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-Installing-Git&#34;&gt;here you are&lt;/a&gt; (and you&amp;rsquo;re a bad person).&lt;/p&gt;

&lt;h1 id=&#34;let-s-start-the-party&#34;&gt;Let&amp;rsquo;s start the party&lt;/h1&gt;

&lt;h2 id=&#34;step-0-install-and-configure-hugo&#34;&gt;Step 0: Install and configure Hugo&lt;/h2&gt;

&lt;p&gt;You only need to follow this steps the first time you use Hugo. Since it is written in go, it supports multiple platforms and comes in pre-built binaries available &lt;a href=&#34;https://github.com/spf13/hugo/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A complete installation guide is available &lt;a href=&#34;https://gohugo.io/overview/installing/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re on a Mac, I advice you install &lt;a href=&#34;http://brew.sh/&#34;&gt;Homebrew&lt;/a&gt; and just &lt;code&gt;brew install hugo&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;before-you-start&#34;&gt;Before you start&lt;/h3&gt;

&lt;p&gt;If you haven&amp;rsquo;t already done that, check that Hugo is correctly configured by running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hugo help
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see something like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hugo is the main command, used to build your Hugo site.

Hugo is a Fast and Flexible Static Site Generator
built with love by spf13 and friends in Go.

Complete documentation is available at http://gohugo.io/.

Usage:
  hugo [flags]
  hugo [command]
  ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It goes on for a while.&lt;/p&gt;

&lt;h2 id=&#34;step-1-create-your-first-hugo-project&#34;&gt;Step 1: Create your first Hugo project&lt;/h2&gt;

&lt;p&gt;In this short example we will create a blog.&lt;/p&gt;

&lt;p&gt;Hugo has a large set of commands to manage your website. To create a new website move to a convenient location and run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hugo new site blog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change directory to &lt;code&gt;blog&lt;/code&gt; and you will see that Hugo has created directories to store all the files required for your website.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drwxr-xr-x  2 user   68 Nov 30 23:58 archetypes
-rw-r--r--  1 user   97 Dec  1 00:22 config.toml
drwxr-xr-x  3 user  102 Nov 30 23:59 content
drwxr-xr-x  2 user   68 Nov 30 23:58 data
drwxr-xr-x  2 user   68 Nov 30 23:58 layouts
drwxr-xr-x  2 user   68 Nov 30 23:58 static
drwxr-xr-x  6 user  204 Dec  1 00:20 themes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The directory has 6 sub-directories and 1 file. Let&amp;rsquo;s look at each one of them:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;archetypes: You can create new content files in Hugo using the &lt;code&gt;hugo new&lt;/code&gt; command.
When you run that command, it adds few configuration properties to the post like date and title.
Archetype allows you to define your own configuration properties that will be added to the post front matter whenever &lt;code&gt;hugo new&lt;/code&gt; command is used.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;config.toml: Every website should have a configuration file at the root.
By default, the configuration file uses &lt;code&gt;TOML&lt;/code&gt; format but you can also use &lt;code&gt;YAML&lt;/code&gt; or &lt;code&gt;JSON&lt;/code&gt; formats as well.
&lt;code&gt;TOML&lt;/code&gt; is minimal configuration file format thatâ€™s easy to read due to obvious semantics.
The configuration settings mentioned in the &lt;code&gt;config.toml&lt;/code&gt; are applied to the full site.
These configuration settings include &lt;code&gt;baseurl&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt; of the website.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;content: This is where you will store content of the website.
Inside &lt;code&gt;content&lt;/code&gt;, you will create sub-directories for different sections.
Letâ€™s suppose your website has three actions â€“ blog, article, and tutorial then you will have three different directories for each of them inside the content directory.
The name of the section i.e. blog, article, or tutorial will be used by Hugo to apply a specific layout applicable to that section.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;data: This directory is used to store configuration files that can be used by Hugo when generating your website.
You can write these files in &lt;code&gt;YAML&lt;/code&gt;, &lt;code&gt;JSON&lt;/code&gt;, or &lt;code&gt;TOML&lt;/code&gt; format.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;layouts: The content inside this directory is used to specify how your content will be converted into the static website.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;static: This directory is used to store all the static content that your website will need like images, CSS, JavaScript or other static content.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;step-2-create-contents&#34;&gt;Step 2: Create contents&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s add a new post to our &lt;code&gt;blog&lt;/code&gt;. We will use the &lt;code&gt;hugo new&lt;/code&gt; command to create a new post.
In our example, we want to create a welcome post to greet users visiting our blog.
Make sure you are inside the &lt;code&gt;blog&lt;/code&gt; directory and type&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hugo new post/welcome
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will create a new directory &lt;code&gt;post&lt;/code&gt; inside the &lt;code&gt;content&lt;/code&gt; directory, and a new file named &lt;code&gt;welcome.md&lt;/code&gt; inside it.&lt;/p&gt;

&lt;p&gt;The content of &lt;code&gt;welcome.md&lt;/code&gt; will look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+++
draft = true
title = &amp;quot;welcome&amp;quot;
date = &amp;quot;2016-12-01T00:22:59+01:00&amp;quot;

+++
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The content inside &lt;code&gt;+++&lt;/code&gt; is the TOML configuration for the post.
This configuration is called &lt;strong&gt;front matter&lt;/strong&gt;.
It enables you to define post configuration along with its content.
By default, each post will have the three configuration properties shown above.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s add a short welcome message:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+++
draft = true
title = &amp;quot;welcome&amp;quot;
date = &amp;quot;2016-12-01T00:22:59+01:00&amp;quot;

+++

Welcome to my blog!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-3-serve-content&#34;&gt;Step 3: Serve content&lt;/h2&gt;

&lt;p&gt;Hugo has an inbuilt server that can serve your website content so that you can preview it.
You can also use the inbuilt Hugo server in production.
Place yourself in the &lt;code&gt;blog&lt;/code&gt; directory and run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hugo server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output will read&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0 of 1 draft rendered
0 future content
0 pages created
0 paginator pages created
0 tags created
0 categories created
in 9 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can view your blog at &lt;a href=&#34;http://localhost:1313&#34;&gt;http://localhost:1313&lt;/a&gt;, go check it out&amp;hellip; it&amp;rsquo;s a blank page.
That&amp;rsquo;s because:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;As you can see in the &lt;code&gt;hugo server&lt;/code&gt; command output, Hugo didnâ€™t render the draft.
Hugo will only render drafts if you pass the &lt;code&gt;buildDrafts&lt;/code&gt; flag to the hugo server command.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We have to specify a theme that Hugo can use to render the content.
We will do that in the next step.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To render drafts, run &lt;code&gt;hugo server --buildDrafts&lt;/code&gt; and head back to &lt;a href=&#34;http://localhost:1313&#34;&gt;http://localhost:1313&lt;/a&gt;.
Yet another wonderful blank page. We are missing the theme, remember that?&lt;/p&gt;

&lt;h2 id=&#34;step-4-add-theme&#34;&gt;Step 4: Add theme&lt;/h2&gt;

&lt;p&gt;Themes provide the layout and templates that will be used by Hugo to render your website.
There are a lot of open-source themes available &lt;a href=&#34;https://themes.gohugo.io/&#34;&gt;here&lt;/a&gt; that you can use.&lt;/p&gt;

&lt;p&gt;In order to add a theme move to the &lt;code&gt;themes&lt;/code&gt; directory inside the root of your website and clone a theme for your website.
For our example, we will use the &lt;code&gt;bleak&lt;/code&gt; theme.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inside the themes directory&lt;/strong&gt; run &lt;code&gt;git clone https://github.com/Zenithar/hugo-theme-bleak.git&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Get back to the &lt;code&gt;blog&lt;/code&gt; directory (&lt;code&gt;cd ..&lt;/code&gt;) and run &lt;code&gt;hugo server --theme=hugo-theme-bleak --buildDrafts&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Go to &lt;a href=&#34;http://localhost:1313&#34;&gt;http://localhost:1313&lt;/a&gt; and enjoy your freshly crafted blog.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s get back to code and try to understand how themes work, in order to customize and modify them.
A theme consist of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;theme.toml&lt;/strong&gt; is the theme configuration file that gives information about the theme like name and description of theme, author details, and theme license.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;images&lt;/strong&gt; directory contains four images, &lt;code&gt;screenshot.png&lt;/code&gt;, &lt;code&gt;tn.png&lt;/code&gt;, &lt;code&gt;full_blog.png&lt;/code&gt; and &lt;code&gt;full_post.png&lt;/code&gt;.
These images are examples of what the theme looks like;
as an example &lt;code&gt;screenshot.png&lt;/code&gt; is the image of the list view and &lt;code&gt;tn.png&lt;/code&gt; is the single post view.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;layouts&lt;/strong&gt; directory contains different views for different content types.
Every content type should have two files &lt;code&gt;single.html&lt;/code&gt; and &lt;code&gt;list.html&lt;/code&gt;.
&lt;code&gt;single.html&lt;/code&gt; is used for rendering a single piece of content.
&lt;code&gt;list.html&lt;/code&gt; is used to view a list of content items.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;static&lt;/strong&gt; directory stores all the static assets used by the template.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Obviously you can clone multiple themes and test theme using the &lt;code&gt;--theme&lt;/code&gt; argument.&lt;/p&gt;

&lt;h2 id=&#34;step-5-update-config-file&#34;&gt;Step 5: Update config file&lt;/h2&gt;

&lt;p&gt;The website uses the dummy values specified in &lt;code&gt;blog/config.toml&lt;/code&gt;.
Letâ€™s update the configuration.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;baseurl = &amp;quot;https://mywebsite.com&amp;quot;
title = &amp;quot;My blog&amp;quot;
languageCode = &amp;quot;en-us&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;optional-add-comments&#34;&gt;Optional: Add comments&lt;/h3&gt;

&lt;p&gt;To integrate comments in your blog you can use &lt;strong&gt;Disqus&lt;/strong&gt;. Just open the &lt;code&gt;blog/config.toml&lt;/code&gt; file and add&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Params]
  disqusShortname = &amp;lt;your disqus shortname&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, commenting will be enabled in your blog.&lt;/p&gt;

&lt;h2 id=&#34;step-6-make-post-public&#34;&gt;Step 6: Make post public&lt;/h2&gt;

&lt;p&gt;Our welcome post is still in &lt;code&gt;draft&lt;/code&gt; status.
To make a draft public, you can either run this command&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hugo undraft content/post/welcome.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or manually change the draft status in the post to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now you can start the server without the &lt;code&gt;buildDrafts&lt;/code&gt; argument.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hugo server --theme=hugo-theme-bleak
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-7-generate-website&#34;&gt;Step 7: Generate website&lt;/h2&gt;

&lt;p&gt;To generate Hugo website source you can use to deploy your website on GitHub pages,
first edit &lt;code&gt;blog/config.toml&lt;/code&gt;, changing the baseurl line to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;baseurl = &amp;quot;https://username.github.io/&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then type&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hugo --theme=hugo-theme-bleak
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After this command, a &lt;code&gt;public&lt;/code&gt; folder will be created, containing teh generated source.&lt;/p&gt;

&lt;h2 id=&#34;step-8-go-public&#34;&gt;Step 8: Go public&lt;/h2&gt;

&lt;p&gt;On the github side of the thing, create a new &lt;strong&gt;empty&lt;/strong&gt; (no README) repo named &lt;code&gt;username.github.io&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now move to the &lt;code&gt;public&lt;/code&gt; directory and add version control.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git init
git remote add origin https://github.com/username/username.github.io.git
git add --all
git commit -m &amp;quot;First commit&amp;quot;
git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visit &lt;strong&gt;&lt;a href=&#34;https://username.github.io/&#34;&gt;https://username.github.io/&lt;/a&gt;&lt;/strong&gt; and enjoy your blog, now available online.&lt;/p&gt;

&lt;p&gt;Anytime, you can regenerate the site with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hugo --theme=hugo-theme-bleak
cd public
git add --all
git commit -m &amp;quot;&amp;lt;some change message&amp;gt;&amp;quot;
git push origin master
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>